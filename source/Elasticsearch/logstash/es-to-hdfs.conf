input {
  elasticsearch {
    hosts => ["47.100.76.204:9200"]
    index => "jc-*2018.05.06"
    codec => "json_lines"
    docinfo => true
  }
}

filter {
  mutate {
    lowercase => [ "channelType", "log_type" ]
    update => ["message", "%{sourceId} %{callAdvTime} %{orderId} %{orderInputId} %{channelType} %{source} %{userid} %{uuid} %{from_ip} %{delMode} %{log_type} %{repeatTime} %{client_ip} %{scid} %{responseMsg} %{orderSourceId} %{channelTime} %{advId} %{ideaId} %{port} %{appid} %{proId} %{status} %{cid} %{delPlat} %{landingPageId} %{clickTime} %{sc_name} %{channelMoney} %{inputMoney} %{notifyChannelUrl} %{callAdvUrl} %{callChannelUrl}"]
    gsub =>["message","%{[A-Za-z_]*}", 'null']
    lowercase => [ "message" ]
    remove_field => ["level", "source","userid","tags","sc_name","log_type","port","thread_name","level_value","appid","@version","client_ip","logger_name","cid","status"]
    add_field => { "type" => "estohdfs" }
  }
}

output {
    if "_grokparsefailure" in [tags] {
      stdout {
        codec => rubydebug
      }
    }

    if "_grokparsefailure" not in [tags] {
      if [type] == "estohdfs" {
        file{ path => "/home/backup/%{+YYYY-MM-dd}/%{[@metadata][_index]}.log" }
        webhdfs {
          host => "192.168.1.83"
          port => 50070
          path => "/online_backup/%{+YYYY-MM-dd}/%{[@metadata][_index]}.log"
          user => "hadoop"
          compression => "snappy"
          snappy_format => "stream"
          id => "estohdfs"
        }
      }
    }
}


```bash
nohup /usr/share/logstash/bin/logstash -f /root/es-to-hdfs.conf --path.data=/tmp/last --path.settings=/etc/logstash/ &
```