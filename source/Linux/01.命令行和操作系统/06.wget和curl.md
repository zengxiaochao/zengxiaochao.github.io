# wget
### 功能

    非交互式网络下载，类似于 HTTP 客户端
### 常用选项：

        -b, --background 后台运行日志记录和输入文件：
        -o, --output-file=FILE 日志写到文件
        -a, --append-output=FILE 日志追加到文件
        -d, --debug 打印 debug 信息，会包含头信息
        -q, --quiet 退出，不输出
        -i, --input-file=FILE 从文件中读取 URL 下载
        下载选项：
        -t, --tries=NUMBER 设置链接重试次数
        -O, --output-document=FILE 写入内容到文件
        -nc, --no-clobber 跳过下载现有的文件
        -c, --continue 断点续传
        --progress=TYPE 设置进度条（dot 和 bar）
        -S, --server-response 打印服务器响应头信息
        --spider 不下载任何内容
        -T, --timeout=SECONDS 设置相应超时时间（还有--dns-timeout、--connect-timeout 和
        --read-timeout）
        -w, --wait=SECONDS 两次重试间隔等待时间
        --bind-address=ADDRESS 设置绑定地址
        --limit-rate=RATE 限制下载速度
        --user=USER 设置 ftp 和 http 用户名
        --password=PASS 设置 ftp 和 http 密码
        目录：
        -P, --directory-prefix=PREFIX 保存文件目录
        HTTP 选项：
        --http-user=USER 设置 http 用户名
        --http-password=PASS 设置 http 密码
        --proxy-user=USER 设置代理用户名
        --proxy-password=PASS 设置代理密码
        --referer=URL 设置 Referer
        --save-headers 保存头到文件
        --default-page=NAME 改变默认页面名字，默认 index.html
        -U,--user-agent=AGENT 设置客户端信息
        --no-http-keep-alive 禁用 HTTP keep-alive（长连接）
        --load-cookies=FILE 从文件加载 cookies
        --save-cookies=FILE 保存 cookies 到文件
        --post-data=STRING 使用 POST 方法，发送数据
        FTP 选项：
        --ftp-user=USER 设置 ftp 用户名
        --ftp-password=PASS 设置 ftp 密码
        --no-passive-ftp 禁用被动传输模式
        递归下载：
        -r, --recursive 指定递归下载
        -l, --level=NUMBER 最大递归深度
        -A, --accept=LIST 逗号分隔下载的扩展列表
        -R, --reject=LIST 逗号分隔不被下载的扩展列表
        -D, --domains=LIST 逗号分隔被下载域的列表
        --exclude-domains=LIST 排除不被下载域的列表
wget参数可以参考如下文章

https://www.bookstack.cn/read/linux-command-1.5.1/command-wget.md?wd=wget



### 示例：

```
下载单个文件到当前目录：
# wget http://nginx.org/download/nginx-1.11.7.tar.gz


放到后台下载：
# wget -b http://nginx.org/download/nginx-1.11.7.tar.gz


对于网络不稳定的用户使用-c 和--tries 参数，保证下载完成，并下载到指定目录：
# wget -t 3 -c http://nginx.org/download/nginx-1.11.7.tar.gz -P down


不下载任何内容，判断 URL 是否可以访问：
# wget --spider http://nginx.org/download/nginx-1.11.7.tar.gz


下载内容写到文件：
# wget http://www.baidu.com/index.html -O index.html


从文件中读取 URL 下载：
# wget -i url.list


下载 ftp 文件：
# wget --ftp-user=admin --ftp-password=admin ftp://192.168.1.10/ISO/CentOS-6.5-i386-
minimal.iso


伪装客户端，指定 user-agent 和 referer 下载：
# wget -U "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)
Chrome/44.0.2403.157 Safari/537.36" --referer "http://nginx.org/en/download.html"
http://nginx.org/download/nginx-1.11.7.tar.gz



查看 HTTP 头信息：
# wget -S http://nginx.org/download/nginx-1.11.7.tar.gz
# wget --debug http://nginx.org/download/nginx-1.11.7.tar.gz
```


## curl
curl 命令的用途广泛，其功能包括下载、发送各种HTTP请求以及指定HTTP头部。

```
      使用下列命令将下载的文件输出到 stdout ：
    $ curl URL
    
      选项 -O 指明将下载数据写入文件，采用从URL中解析出的文件名。注意，其中的URL必
    须是完整的，不能仅是站点的域名：
    $ curl www.knopper.net/index.htm --silent -O
    
      选项 -- silent可以让 curl 命令不显示进度信息：
    $ curl URL --silent
    
      如果需要在下载过程中显示形如 # 的进度条，可以使用选项 --progress ：
    $ curl http://knopper.net -o index.html --progress
    
    1. 断点续传
    $ curl URL/file -C offset
    
```

## shell爬取网站图片的用例
``` 
#!/bin/bash
#用途:图片下载工具
#文件名: img_downloader.sh
if [ $# -ne 3 ];
then
    echo "Usage: $0 URL -d DIRECTORY"
    exit -1
fi


while [ $# -gt 0 ]
do
    case $1 in
        -d) shift; directory=$1; shift ;;
        *) url=$1; shift;;
esac
done

mkdir -p $directory;
baseurl=$(echo $url | egrep -o "https?://[a-z.\-]+")
echo Downloading $url

curl -s $url | egrep -o "<img[^>]*src=[^>]*>" | \
sed 's/<img[^>]*src=\"\([^"]*\).*/\1/g' | \
sed "s,^/,$baseurl/," > /tmp/$$.list

cd $directory;

while read filename;
do
    echo Downloading $filename
    curl -s -O "$filename" --silent
done < /tmp/$$.list
```

``` 
while [ -n "$1" ]
do
    case $1 in
    -d) shift; directory=$1; shift ;;
    *) url=${url:-$1}; shift;;
esac
don

while 循环会一直处理完所有的参数。 shift 用来向左移动参数，这样 $2 的值就会被赋给  $1，
$3 的值被赋给  $2 ，往后以此类推。因此通过  $1 就可以求值所有的参数

case 语句检查第一个参数（ $1 ）。如果匹配 -d ，那么下一个参数一定是目录，接着就移动参
数并保存目录名。否则的话，就是 URL 。
采用这种方法来解析命令行参数的好处在于可以将 -d 置于命令行中的任意位置：


$ ./img_downloader.sh -d DIR URL
或者
$ ./img_downloader.sh URL -d DIR
```





#### wget 命令

```
wget url #下载文件
wget -O demo.html url #下载文件并改为指定名称
wget -c url #断点续传
wget -i url url url #下载多个文件
wget -t 5 url #网络不好时重试 5 次
wget -t 0 url #不断重试
wget --limit-rate 20k url #限速下载
wget -Q 100m url #设置下载最大额度
wget --mirror --convert-links url #递归复制其他网站作为镜像，类似爬虫
wget --user username --password pass url #认证
```

补充：`*wget*` 命令适用于下载。



#### curl 命令

```
curl ifconfig.me #查看外网 IP

curl www.baidu.com #请求指定网址并显示出来

curl -o demo.html https://www.baidu.com/index.php #下载指定文件并重命名为 demo.html

curl -v www.baidu.com #显示请求的过程信息

curl url --silent #不显示下载进度信息

curl url -o index.html --progress #显示下载进度信息

curl --referer Referer_URL target_URL #设置 referer

curl url --cookie "user=username;pass=password" #设置 cookie 

curl url --user-agent "Mozilla/5.0 ..." #设置 User-Agent

curl url --limit-rate 20k  #限速

curl url --max-filesize bytes #设置下载最大额度

curl url -H 'HOST: 1.1.1.1' -H 'X-Real-IP: 2.2.2.2' #设置 http header
curl -X GET www.baidu.com #指定请求方法，请求方法必须大写

curl -C url #断点续传，适用网络不顺畅时下载大文件，无须重新下载

curl --connect-timeout 20 https://wp.hellocode.name/?p=701 #指定 url 并限制连接时间
```

补充：`curl` 命令适用于上传与请求。



## 这篇文章最全的介绍了wget和curl

https://www.cnblogs.com/taosim/articles/4092572.html